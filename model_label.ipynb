{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carbarcha/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "tweet_model = pipeline(task=\"text-classification\", model=\"./roberta-base_twitter\")\n",
    "imdb_model = pipeline(task=\"text-classification\", model=\"./roberta-base_imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9983030557632446}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_model(\"I love this movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_text(text, model=\"tweet\"):\n",
    "    if model == \"tweet\":\n",
    "        return tweet_model(text)\n",
    "    elif model == \"imdb\":\n",
    "        return imdb_model(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare news document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "delimiters = \"[， 。]\"\n",
    "\n",
    "def split_text_by_length(text, max_length=500):\n",
    "    words = re.split(delimiters, text)\n",
    "    split_texts = []\n",
    "    current_part = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        if current_length + len(word) + 1 <= max_length:\n",
    "            current_part.append(word)\n",
    "            current_length += len(word) + 1\n",
    "        else:\n",
    "            split_texts.append(' '.join(current_part))\n",
    "            current_part = [word]\n",
    "            current_length = len(word) + 1\n",
    "\n",
    "    if current_part:\n",
    "        split_texts.append(' '.join(current_part))\n",
    "\n",
    "    return split_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All news data count : 19506\n",
      "process data : 50\n",
      "process data : 100\n",
      "process data : 150\n",
      "process data : 200\n",
      "process data : 250\n",
      "process data : 300\n",
      "process data : 350\n",
      "process data : 400\n",
      "process data : 450\n",
      "process data : 500\n",
      "process data : 550\n",
      "process data : 600\n",
      "process data : 650\n",
      "process data : 700\n",
      "process data : 750\n",
      "process data : 800\n",
      "process data : 850\n",
      "process data : 900\n",
      "process data : 950\n",
      "process data : 1000\n",
      "process data : 1050\n",
      "process data : 1100\n",
      "process data : 1150\n",
      "process data : 1200\n",
      "process data : 1250\n",
      "process data : 1300\n",
      "process data : 1350\n",
      "process data : 1400\n",
      "process data : 1450\n",
      "process data : 1500\n",
      "process data : 1550\n",
      "process data : 1600\n",
      "process data : 1650\n",
      "process data : 1700\n",
      "process data : 1750\n",
      "process data : 1800\n",
      "process data : 1850\n",
      "process data : 1900\n",
      "process data : 1950\n",
      "process data : 2000\n",
      "process data : 2050\n",
      "process data : 2100\n",
      "process data : 2150\n",
      "process data : 2200\n",
      "process data : 2250\n",
      "process data : 2300\n",
      "process data : 2350\n",
      "process data : 2400\n",
      "process data : 2450\n",
      "process data : 2500\n",
      "process data : 2550\n",
      "process data : 2600\n",
      "process data : 2650\n",
      "process data : 2700\n",
      "process data : 2750\n",
      "process data : 2800\n",
      "process data : 2850\n",
      "process data : 2900\n",
      "process data : 2950\n",
      "process data : 3000\n",
      "process data : 3050\n",
      "process data : 3100\n",
      "process data : 3150\n",
      "process data : 3200\n",
      "process data : 3250\n",
      "process data : 3300\n",
      "process data : 3350\n",
      "process data : 3400\n",
      "process data : 3450\n",
      "process data : 3500\n",
      "process data : 3550\n",
      "process data : 3600\n",
      "process data : 3650\n",
      "process data : 3700\n",
      "process data : 3750\n",
      "process data : 3800\n",
      "process data : 3850\n",
      "process data : 3900\n",
      "process data : 3950\n",
      "process data : 4000\n",
      "process data : 4050\n",
      "process data : 4100\n",
      "process data : 4150\n",
      "process data : 4200\n",
      "process data : 4250\n",
      "process data : 4300\n",
      "process data : 4350\n",
      "process data : 4400\n",
      "process data : 4450\n",
      "process data : 4500\n",
      "process data : 4550\n",
      "process data : 4600\n",
      "process data : 4650\n",
      "process data : 4700\n",
      "process data : 4750\n",
      "process data : 4800\n",
      "process data : 4850\n",
      "process data : 4900\n",
      "process data : 4950\n",
      "process data : 5000\n",
      "process data : 5050\n",
      "process data : 5100\n",
      "process data : 5150\n",
      "process data : 5200\n",
      "process data : 5250\n",
      "process data : 5300\n",
      "process data : 5350\n",
      "process data : 5400\n",
      "process data : 5450\n",
      "process data : 5500\n",
      "process data : 5550\n",
      "process data : 5600\n",
      "process data : 5650\n",
      "process data : 5700\n",
      "process data : 5750\n",
      "process data : 5800\n",
      "process data : 5850\n",
      "process data : 5900\n",
      "process data : 5950\n",
      "process data : 6000\n",
      "process data : 6050\n",
      "process data : 6100\n",
      "process data : 6150\n",
      "process data : 6200\n",
      "process data : 6250\n",
      "process data : 6300\n",
      "process data : 6350\n",
      "process data : 6400\n",
      "process data : 6450\n",
      "process data : 6500\n",
      "process data : 6550\n",
      "process data : 6600\n",
      "process data : 6650\n",
      "process data : 6700\n",
      "process data : 6750\n",
      "process data : 6800\n",
      "process data : 6850\n",
      "process data : 6900\n",
      "process data : 6950\n",
      "process data : 7000\n",
      "process data : 7050\n",
      "process data : 7100\n",
      "process data : 7150\n",
      "process data : 7200\n",
      "process data : 7250\n",
      "process data : 7300\n",
      "process data : 7350\n",
      "process data : 7400\n",
      "process data : 7450\n",
      "process data : 7500\n",
      "process data : 7550\n",
      "process data : 7600\n",
      "process data : 7650\n",
      "process data : 7700\n",
      "process data : 7750\n",
      "process data : 7800\n",
      "process data : 7850\n",
      "process data : 7900\n",
      "process data : 7950\n",
      "process data : 8000\n",
      "process data : 8050\n",
      "process data : 8100\n",
      "process data : 8150\n",
      "process data : 8200\n",
      "process data : 8250\n",
      "process data : 8300\n",
      "process data : 8350\n",
      "process data : 8400\n",
      "process data : 8450\n",
      "process data : 8500\n",
      "process data : 8550\n",
      "process data : 8600\n",
      "process data : 8650\n",
      "process data : 8700\n",
      "process data : 8750\n",
      "process data : 8800\n",
      "process data : 8850\n",
      "process data : 8900\n",
      "process data : 8950\n",
      "process data : 9000\n",
      "process data : 9050\n",
      "process data : 9100\n",
      "process data : 9150\n",
      "process data : 9200\n",
      "process data : 9250\n",
      "process data : 9300\n",
      "process data : 9350\n",
      "process data : 9400\n",
      "process data : 9450\n",
      "process data : 9500\n",
      "process data : 9550\n",
      "process data : 9600\n",
      "process data : 9650\n",
      "process data : 9700\n",
      "process data : 9750\n",
      "process data : 9800\n",
      "process data : 9850\n",
      "process data : 9900\n",
      "process data : 9950\n",
      "process data : 10000\n",
      "process data : 10050\n",
      "process data : 10100\n",
      "process data : 10150\n",
      "process data : 10200\n",
      "process data : 10250\n",
      "process data : 10300\n",
      "process data : 10350\n",
      "process data : 10400\n",
      "process data : 10450\n",
      "process data : 10500\n",
      "process data : 10550\n",
      "process data : 10600\n",
      "process data : 10650\n",
      "process data : 10700\n",
      "process data : 10750\n",
      "process data : 10800\n",
      "process data : 10850\n",
      "process data : 10900\n",
      "process data : 10950\n",
      "process data : 11000\n",
      "process data : 11050\n",
      "process data : 11100\n",
      "process data : 11150\n",
      "process data : 11200\n",
      "process data : 11250\n",
      "process data : 11300\n",
      "process data : 11350\n",
      "process data : 11400\n",
      "process data : 11450\n",
      "process data : 11500\n",
      "process data : 11550\n",
      "process data : 11600\n",
      "process data : 11650\n",
      "process data : 11700\n",
      "process data : 11750\n",
      "process data : 11800\n",
      "process data : 11850\n",
      "process data : 11900\n",
      "process data : 11950\n",
      "process data : 12000\n",
      "process data : 12050\n",
      "process data : 12100\n",
      "process data : 12150\n",
      "process data : 12200\n",
      "process data : 12250\n",
      "process data : 12300\n",
      "process data : 12350\n",
      "process data : 12400\n",
      "process data : 12450\n",
      "process data : 12500\n",
      "process data : 12550\n",
      "process data : 12600\n",
      "process data : 12650\n",
      "process data : 12700\n",
      "process data : 12750\n",
      "process data : 12800\n",
      "process data : 12850\n",
      "process data : 12900\n",
      "process data : 12950\n",
      "process data : 13000\n",
      "process data : 13050\n",
      "process data : 13100\n",
      "process data : 13150\n",
      "process data : 13200\n",
      "process data : 13250\n",
      "process data : 13300\n",
      "process data : 13350\n",
      "process data : 13400\n",
      "process data : 13450\n",
      "process data : 13500\n",
      "process data : 13550\n",
      "process data : 13600\n",
      "process data : 13650\n",
      "process data : 13700\n",
      "process data : 13750\n",
      "process data : 13800\n",
      "process data : 13850\n",
      "process data : 13900\n",
      "process data : 13950\n",
      "process data : 14000\n",
      "process data : 14050\n",
      "process data : 14100\n",
      "process data : 14150\n",
      "process data : 14200\n",
      "process data : 14250\n",
      "process data : 14300\n",
      "process data : 14350\n",
      "process data : 14400\n",
      "process data : 14450\n",
      "process data : 14500\n",
      "process data : 14550\n",
      "process data : 14600\n",
      "process data : 14650\n",
      "process data : 14700\n",
      "process data : 14750\n",
      "process data : 14800\n",
      "process data : 14850\n",
      "process data : 14900\n",
      "process data : 14950\n",
      "process data : 15000\n",
      "process data : 15050\n",
      "process data : 15100\n",
      "process data : 15150\n",
      "process data : 15200\n",
      "process data : 15250\n",
      "process data : 15300\n",
      "process data : 15350\n",
      "process data : 15400\n",
      "process data : 15450\n",
      "process data : 15500\n",
      "process data : 15550\n",
      "process data : 15600\n",
      "process data : 15650\n",
      "process data : 15700\n",
      "process data : 15750\n",
      "process data : 15800\n",
      "process data : 15850\n",
      "process data : 15900\n",
      "process data : 15950\n",
      "process data : 16000\n",
      "process data : 16050\n",
      "process data : 16100\n",
      "process data : 16150\n",
      "process data : 16200\n",
      "process data : 16250\n",
      "process data : 16300\n",
      "process data : 16350\n",
      "process data : 16400\n",
      "process data : 16450\n",
      "process data : 16500\n",
      "process data : 16550\n",
      "process data : 16600\n",
      "process data : 16650\n",
      "process data : 16700\n",
      "process data : 16750\n",
      "process data : 16800\n",
      "process data : 16850\n",
      "process data : 16900\n",
      "process data : 16950\n",
      "process data : 17000\n",
      "process data : 17050\n",
      "process data : 17100\n",
      "process data : 17150\n",
      "process data : 17200\n",
      "process data : 17250\n",
      "process data : 17300\n",
      "process data : 17350\n",
      "process data : 17400\n",
      "process data : 17450\n",
      "process data : 17500\n",
      "process data : 17550\n",
      "process data : 17600\n",
      "process data : 17650\n",
      "process data : 17700\n",
      "process data : 17750\n",
      "process data : 17800\n",
      "process data : 17850\n",
      "process data : 17900\n",
      "process data : 17950\n",
      "process data : 18000\n",
      "process data : 18050\n",
      "process data : 18100\n",
      "process data : 18150\n",
      "process data : 18200\n",
      "process data : 18250\n",
      "process data : 18300\n",
      "process data : 18350\n",
      "process data : 18400\n",
      "process data : 18450\n",
      "process data : 18500\n",
      "process data : 18550\n",
      "process data : 18600\n",
      "process data : 18650\n",
      "process data : 18700\n",
      "process data : 18750\n",
      "process data : 18800\n",
      "process data : 18850\n",
      "process data : 18900\n",
      "process data : 18950\n",
      "process data : 19000\n",
      "process data : 19050\n",
      "process data : 19100\n",
      "process data : 19150\n",
      "process data : 19200\n",
      "process data : 19250\n",
      "process data : 19300\n",
      "process data : 19350\n",
      "process data : 19400\n",
      "process data : 19450\n",
      "process data : 19500\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "\n",
    "file_path = './news_data/origin/abm.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    news_collection = json.load(file)\n",
    "\n",
    "format_str = \"%Y-%m-%d\"\n",
    "tweets_record = []\n",
    "\n",
    "print(f\"All news data count : {len(news_collection)}\")\n",
    "\n",
    "for index, tweet in enumerate(news_collection):\n",
    "    date = parser.parse(tweet[\"time\"]).strftime(format_str)\n",
    "    news_detail = {\n",
    "        \"date\":date,\n",
    "    }\n",
    "    content = tweet[\"content\"]\n",
    "    res = analysis_text(split_text_by_length(content))\n",
    "\n",
    "    # 這邊直接判斷哪種情緒出現在文章比較多\n",
    "    bullish = 0\n",
    "    bearish = 0\n",
    "    for data in res:\n",
    "        label = data[\"label\"]\n",
    "        if label == \"LABEL_1\":\n",
    "            bullish += 1\n",
    "        elif label == \"LABEL_0\":\n",
    "            bearish += 1\n",
    "    if bullish > bearish:\n",
    "        news_detail[\"label\"]=\"bullish\"\n",
    "    elif bearish > bullish:\n",
    "        news_detail[\"label\"]=\"bearish\"\n",
    "    else:\n",
    "        news_detail[\"label\"]=\"neutral\"\n",
    "    if (index+1)%50==0:\n",
    "        print(f\"process data : {index+1}\")\n",
    "    tweets_record.append(news_detail)\n",
    "\n",
    "df = pd.DataFrame(tweets_record)\n",
    "df.to_json(\"./news_data/news_with_label/abm.json\", orient=\"records\", indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare twitter document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tweet data count : 73\n",
      "process data : 50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "\n",
    "file_path = './tweeter_data/filter/VitalikButerin.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    news_collection = json.load(file)\n",
    "\n",
    "format_str = \"%Y-%m-%d\"\n",
    "tweets_record = []\n",
    "\n",
    "print(f\"All tweet data count : {len(news_collection)}\")\n",
    "\n",
    "for index, tweet in enumerate(news_collection):\n",
    "    date = parser.parse(tweet[\"date\"]).strftime(format_str)\n",
    "    news_detail = {\n",
    "        \"date\":date,\n",
    "    }\n",
    "    tweet_text = tweet.get(\"tweet_text\", \"\")\n",
    "    repost_text = tweet.get(\"repost_text\", \"\")\n",
    "    content = \"\"\n",
    "    if tweet_text is not None:\n",
    "        content += f\"{tweet_text}\\n\"\n",
    "    if repost_text is not None:\n",
    "        content += f\"{repost_text}\\n\"\n",
    "    res = analysis_text(text=content, model=\"imdb\")\n",
    "\n",
    "    bullish = 0\n",
    "    bearish = 0\n",
    "    for data in res:\n",
    "        label = data[\"label\"]\n",
    "        if label == \"LABEL_1\":\n",
    "            bullish += 1\n",
    "        elif label == \"LABEL_0\":\n",
    "            bearish += 1\n",
    "    if bullish > bearish:\n",
    "        news_detail[\"label\"]=\"bullish\"\n",
    "    elif bearish > bullish:\n",
    "        news_detail[\"label\"]=\"bearish\"\n",
    "    else:\n",
    "        news_detail[\"label\"]=\"neutral\"\n",
    "    if (index+1)%50==0:\n",
    "        print(f\"process data : {index+1}\")\n",
    "    tweets_record.append(news_detail)\n",
    "\n",
    "df = pd.DataFrame(tweets_record)\n",
    "df.to_json(\"./tweeter_data/tweeter_with_label/VitalikButerin.json\", orient=\"records\", indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
